import sqlite3
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# **ğŸ“Œ è¨­å®š SQLite è³‡æ–™åº«**
DB_PATH = "rag_data.db"

# **ğŸ“Œ è®€å– CSV æª”æ¡ˆ**
csv_file_1 = "E:\\python_project\\contest\\poster_fukui\\google_comment\\google_new_comment_1\\all_tripadviser_en_nlp.csv"
csv_file_2 = "E:\\python_project\\contest\\poster_fukui\\google_comment\\google_new_comment\\all_jalan_en_nlp.csv"

df1 = pd.read_csv(csv_file_1)
df2 = pd.read_csv(csv_file_2)
df = pd.concat([df1, df2], ignore_index=True)

# **ğŸ“Œ åˆå§‹åŒ– Sentence-BERT**
sbert_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# **ğŸ“Œ ç”Ÿæˆèªç¾©å‘é‡**
df["embeddings"] = df["translated_comment"].apply(lambda x: sbert_model.encode(x, convert_to_numpy=True))
dimension = df["embeddings"][0].shape[0]

# **ğŸ“Œ å»ºç«‹ FAISS å‘é‡ç´¢å¼•**
index = faiss.IndexFlatL2(dimension)
embeddings_matrix = np.vstack(df["embeddings"].to_numpy())
index.add(embeddings_matrix)

# **ğŸ“Œ å„²å­˜ FAISS Index**
faiss.write_index(index, "faiss_index.bin")

# **ğŸ“Œ é€£æ¥ SQLite**
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# **ğŸ“Œ å»ºç«‹è³‡æ–™è¡¨**
cursor.execute('''
    CREATE TABLE IF NOT EXISTS reviews (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        location TEXT,
        translated_comment TEXT,
        keyword TEXT,
        adj TEXT
    )
''')

# **ğŸ“Œ æ’å…¥è³‡æ–™**
df[["location", "translated_comment", "keyword", "adj"]].to_sql("reviews", conn, if_exists="replace", index=False)

# **ğŸ“Œ ç¢ºä¿è®Šæ›´å„²å­˜**
conn.commit()
conn.close()

print("âœ… CSV è³‡æ–™å·²å­˜å…¥ SQLiteï¼ŒFAISS Index å·²å»ºç«‹")
